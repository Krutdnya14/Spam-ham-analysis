{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd7293b-9816-42b7-b911-0f7dfcea8a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    " nltk.download('brown')\n",
    " nltk.download('punkt')\n",
    " nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9a0c0b2-74ad-4513-a610-1d0d0fd1367d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john\n",
      "natural language processing\n"
     ]
    }
   ],
   "source": [
    "# extract noun phrases.\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"John is learning natural language processing\")\n",
    "for np in blob.noun_phrases:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93de0598-2b9f-48bd-8f0a-e220e5e5113e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (5, 10)\n",
      "\n",
      "Cosine similarity of first document with all documents:\n",
      "[[1.         0.17682765 0.14284054 0.13489366 0.68374784]]\n",
      "\n",
      "Hamming similarity of first document with all documents:\n",
      "[1.0, 0.7, 0.6, 0.6, 0.9]\n"
     ]
    }
   ],
   "source": [
    "# Finding Similarit Between Texts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import hamming\n",
    "import numpy as np\n",
    "documents = (\n",
    " \"I like NLP\",\n",
    " \"I am exploring NLP\",\n",
    " \"I am a beginner in NLP\",\n",
    " \"I want to learn NLP\",\n",
    " \"I like advanced NLP\"\n",
    " )\n",
    " # Cosine Similarity (TF-IDF)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "cosine_scores = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "print(\"\\nCosine similarity of first document with all documents:\")\n",
    "print(cosine_scores)\n",
    " # Hamming Similarity (Binary Bag of Words)\n",
    "count_vectorizer = CountVectorizer(binary=True)\n",
    "binary_matrix = count_vectorizer.fit_transform(documents).toarray()\n",
    "hamming_scores = []\n",
    "for i in range(len(documents)):\n",
    "    dist = hamming(binary_matrix[0], binary_matrix[i])\n",
    "    sim = 1- dist # similarity = 1- distance\n",
    "    hamming_scores.append(sim)\n",
    "print(\"\\nHamming similarity of first document with all documents:\")\n",
    "print(hamming_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f57a6b5-d326-4b0a-a9d3-1e193b6edabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('love', 'NN'), ('NLP', 'NNP'), ('learn', 'NN'), ('NLP', 'NNP'), ('2', 'CD'), ('month', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Tagging Part of Speech\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "Text = \"I love NLP and I will learn NLP in 2 month\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sentences = sent_tokenize(Text)\n",
    "all_tags = []\n",
    "for sent in sentences:\n",
    "    words = word_tokenize(sent)\n",
    "    words = [w for w in words if w.lower() not in stop_words]\n",
    "    tags = nltk.pos_tag(words)\n",
    "    all_tags.extend(tags)\n",
    "print(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f03257e-ac6c-480b-8279-494a659279ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am learning NLP, it is very interesting and exciting.it includes machine learning and deep learning',\n",
       " 'My father is a data scientist and he is nlp expert',\n",
       " 'My sister has good exposure into android development']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Extracting Topics from Text\n",
    "doc1 = '''I am learning NLP, it is very interesting and exciting.it includes machine learning and deep learning'''\n",
    "doc2 = \"My father is a data scientist and he is nlp expert\"\n",
    "doc3 = \"My sister has good exposure into android development\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7edb0e15-b3d1-4f9b-8291-41bc3d84edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude) # fixed␣↪here\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78e7465b-e871-40d3-b6af-6c14e39b84eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['learning',\n",
       "  'nlp',\n",
       "  'interesting',\n",
       "  'excitingit',\n",
       "  'includes',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'deep',\n",
       "  'learning'],\n",
       " ['father', 'data', 'scientist', 'nlp', 'expert'],\n",
       " ['sister', 'good', 'exposure', 'android', 'development']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a2e493d-e0c7-4d63-a5e6-472cc089cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.129*\"nlp\" + 0.129*\"expert\" + 0.129*\"data\" + 0.129*\"scientist\" + 0.129*\"father\" + 0.032*\"exposure\" + 0.032*\"good\" + 0.032*\"development\" + 0.032*\"android\" + 0.032*\"sister\"'), (1, '0.173*\"learning\" + 0.069*\"deep\" + 0.069*\"includes\" + 0.069*\"machine\" + 0.069*\"excitingit\" + 0.069*\"interesting\" + 0.069*\"android\" + 0.069*\"good\" + 0.069*\"exposure\" + 0.069*\"development\"'), (2, '0.063*\"nlp\" + 0.063*\"exposure\" + 0.063*\"android\" + 0.063*\"sister\" + 0.063*\"good\" + 0.063*\"development\" + 0.062*\"interesting\" + 0.062*\"deep\" + 0.062*\"excitingit\" + 0.062*\"machine\"')]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "doc_term_matrix\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "print(ldamodel.print_topics())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e672005d-37b4-4130-aeda-1870ad6323a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spam / Ham Classification\n",
    "import pandas as pd\n",
    "#Read the data\n",
    "Email_Data = pd.read_csv(r\"C:\\Users\\hp\\Downloads\\spam.csv\",encoding ='latin1')\n",
    "\n",
    "#Data undestanding\n",
    "Email_Data.columns\n",
    "\n",
    "Email_Data = Email_Data[['v1', 'v2']]\n",
    "Email_Data = Email_Data.rename(columns={\"v1\":\"Target\", \"v2\":\"Email\"})\n",
    "Email_Data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cbb28fb3-1228-4f9a-8499-08ff70aff4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point, crazy.. avail bugi n great wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joke wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor... u c alreadi say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goe usf, live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  go jurong point, crazy.. avail bugi n great wo...\n",
       "1    ham                        ok lar... joke wif u oni...\n",
       "2   spam  free entri 2 wkli comp win fa cup final tkt 21...\n",
       "3    ham          u dun say earli hor... u c alreadi say...\n",
       "4    ham              nah think goe usf, live around though"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6-2 Text processing and feature engineering\n",
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "\n",
    "#pre processing steps like lower case, stemming and lemmatization\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x:\n",
    " \" \".join(x.lower() for x in x.split()))\n",
    "stop = stopwords.words('english')\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join\n",
    " (x for x in x.split() if x not in stop))\n",
    "st = PorterStemmer()\n",
    "\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join\n",
    " ([st.stem(word) for word in x.split()]))\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join\n",
    " ([Word(word).lemmatize() for word in\n",
    " x.split()]))\n",
    "Email_Data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "22a71531-b434-452e-8071-f48417dcf252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train and validation\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(Email_Data['Email'], Email_Data['Target'])\n",
    "\n",
    "# TFIDF feature generation for a maximum of 5000 features\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(Email_Data['Email'])\n",
    "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
    "xtrain_tfidf.data\n",
    "\n",
    "# Model training\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    return metrics.accuracy_score(predictions, valid_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5164da58-34ca-4af7-a98b-eb3429fdc7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9877961234745154\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes training\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(alpha=0.2), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e911323-be76-487f-af1f-9b91c981cc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9720028715003589\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e141e2-4294-4ebd-91a0-24e783af4e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
